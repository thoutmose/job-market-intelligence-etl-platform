# Job Market Intelligence ETL Platform

[![Python Version](https://img.shields.io/badge/python-3.13%2B-blue.svg)](https://www.python.org/downloads/)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-3.1.5-017CEE?logo=apache-airflow)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-316192?logo=postgresql)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)

**ğŸŒ Language / Langue:** [English](README.md) | [FranÃ§ais](README.fr.md)

---

A comprehensive data engineering platform for extracting, transforming, and loading job market data using Apache Airflow. This project automates the collection of job postings from external APIs, enriches them with technical skills, benefits, and location data, and stores them in a dimensional data warehouse for business intelligence and visualization.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   JSearch API   â”‚
â”‚  External Job   â”‚
â”‚   Data Source   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Extract
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache Airflow  â”‚
â”‚  ETL Pipeline   â”‚â—„â”€â”€â”€ Transform
â”‚  Orchestration  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Load
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚
â”‚ Data Warehouse  â”‚
â”‚ (Star Schema)   â”‚
â”‚ Remote Database â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Query
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Apache Superset  â”‚
â”‚ Visualization   â”‚
â”‚  Remote BI Tool â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

- **JSearch API**: External job posting data source
- **Apache Airflow**: Orchestration engine for ETL workflows
- **PostgreSQL**: Dimensional data warehouse (star schema)
- **Apache Superset**: Business intelligence and data visualization platform (remote)

### Infrastructure

The platform is deployed across three virtual machines:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VM 1: Airflow Server                       â”‚
â”‚                     (Orchestration & Workflow)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Apache Airflow (webserver, scheduler, workers)                  â”‚
â”‚  â€¢ DAG Repository                                                  â”‚
â”‚  â€¢ Python ETL Scripts                                              â”‚
â”‚  â€¢ Docker Containers                                               â”‚
â”‚  â€¢ Connects to: JSearch API, PostgreSQL (VM2), Superset (VM3)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ Extract, Transform, Load
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       VM 2: PostgreSQL Server                      â”‚
â”‚                         (Data Warehouse)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ PostgreSQL 16 Database                                          â”‚
â”‚  â€¢ Star Schema (fact_job_post + dimensions)                        â”‚
â”‚  â€¢ Job posting data storage                                        â”‚
â”‚  â€¢ Serves data to: Airflow (VM1), Superset (VM3)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ Query & Visualize
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VM 3: Apache Superset Server                   â”‚
â”‚                    (BI & Visualization Layer)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Apache Superset Dashboard                                       â”‚
â”‚  â€¢ Interactive Charts & Reports                                    â”‚
â”‚  â€¢ SQL Lab for Ad-hoc Analysis                                     â”‚
â”‚  â€¢ Reads data from: PostgreSQL (VM2)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Infrastructure Details:**
- **VM 1 (Airflow)**: Orchestrates ETL workflows, extracts data from JSearch API, transforms and loads to PostgreSQL
- **VM 2 (PostgreSQL)**: Centralized data warehouse with dimensional model (star schema)
- **VM 3 (Superset)**: Business intelligence platform for data exploration and visualization

## ğŸ“Š Data Model

The platform implements a **star schema** with the following structure:

```
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚    dim_date      â”‚
                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                           â”‚ date_key (PK)    â”‚
                           â”‚ full_date        â”‚
                           â”‚ year             â”‚
                           â”‚ quarter          â”‚
                           â”‚ month            â”‚
                           â”‚ month_name       â”‚
                           â”‚ day              â”‚
                           â”‚ day_of_week      â”‚
                           â”‚ day_name         â”‚
                           â”‚ week_of_year     â”‚
                           â”‚ is_weekend       â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dim_employer    â”‚                â”‚               â”‚  dim_location    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ employer_key (PK)â”‚                â”‚               â”‚ location_key (PK)â”‚
â”‚ employer_name    â”‚                â”‚               â”‚ job_city         â”‚
â”‚ publisher        â”‚                â”‚               â”‚ job_country      â”‚
â”‚ industry         â”‚                â”‚               â”‚ job_region       â”‚
â”‚ company_size     â”‚                â”‚               â”‚ continent        â”‚
â”‚ founded_year     â”‚                â”‚               â”‚ latitude         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚               â”‚ longitude        â”‚
         â”‚                          â”‚               â”‚ postcode         â”‚
         â”‚                          â”‚               â”‚ isocode3166      â”‚
         â”‚                          â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚                        â”‚
         â”‚                          â–¼                        â”‚
         â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  fact_job_post       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â”‚ job_key (PK)         â”‚
                         â”‚ date_key (FK)        â”‚
                         â”‚ location_key (FK)    â”‚
                         â”‚ employer_key (FK)    â”‚
                         â”‚ job_id               â”‚
                         â”‚ job_title            â”‚
                         â”‚ apply_link           â”‚
                         â”‚ employment_type      â”‚
                         â”‚ posted_timestamp     â”‚
                         â”‚ job_salary           â”‚
                         â”‚ job_min_salary       â”‚
                         â”‚ job_max_salary       â”‚
                         â”‚ technologies_list    â”‚
                         â”‚ tools_list           â”‚
                         â”‚ benefits_list        â”‚
                         â”‚ seniority_levels_listâ”‚
                         â”‚ technology_count     â”‚
                         â”‚ tools_count          â”‚
                         â”‚ benefits_count       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fact Table
- `fact_job_post`: Central fact table containing job posting metrics and foreign keys to all dimensions

### Dimension Tables
- `dim_date`: Time dimension with hierarchies (year, quarter, month, week, day)
- `dim_location`: Geographic dimension (city, country, region, postal code, ISO codes)
- `dim_employer`: Employer/company dimension with metadata

## ğŸš€ Features

### ETL Pipeline Capabilities

1. **Extraction**
   - API health check sensor to ensure data source availability
   - Automated job posting retrieval from JSearch API
   - Configurable search parameters (location, date range, number of pages)

2. **Transformation**
   - **Hard Skills Detection**: Identifies technologies and tools mentioned in job descriptions
     - Machine Learning & AI landscape (from MAD landscape)
     - Programming languages and frameworks
     - Data engineering tools
   - **Location Enrichment**: 
     - Postal code lookup from INSEE data
     - ISO 3166-2 region code generation
   - **Seniority Level Extraction**: Detects experience requirements
   - **Salary Information**: Extracts mentioned salary ranges
   - **Benefits Detection**: Identifies perks like remote work, health insurance, meal vouchers, etc.

3. **Loading**
   - Dimensional modeling with surrogate keys
   - Upsert logic (handles duplicates)
   - Referential integrity maintenance
   - Transaction management with rollback on errors

## ğŸ› ï¸ Technology Stack

- **Orchestration**: Apache Airflow 3.1.5
- **Task Distribution**: Celery with Redis broker
- **Database**: PostgreSQL 16
- **Data Visualization**: Apache Superset (remote)
- **Containerization**: Docker & Docker Compose
- **Language**: Python 3.13+

## ğŸ“ Project Structure

```
job-market-intelligence-etl-platform/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ job_post_dag.py          # Main ETL DAG definition
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mad_landscape.json       # ML/AI tools reference
â”‚   â”œâ”€â”€ technologies.json        # Tech stack reference
â”‚   â””â”€â”€ post_code_insee.csv      # French postal codes
â”œâ”€â”€ config/
â”‚   â””â”€â”€ airflow.cfg              # Airflow configuration
â”œâ”€â”€ logs/                         # Airflow execution logs
â”œâ”€â”€ plugins/                      # Custom Airflow plugins
â”œâ”€â”€ include/                      # Additional resources
â”œâ”€â”€ docker-compose.yaml          # Multi-container orchestration
â””â”€â”€ pyproject.toml               # Python project metadata
```

## ğŸ”§ Setup and Installation

### Prerequisites

- Docker and Docker Compose
- At least 4GB RAM
- At least 2 CPU cores
- 10GB free disk space

### Installation Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/thoutmose/job-market-intelligence-etl-platform
   cd job-market-intelligence-etl-platform
   ```

2. **Create environment file**
   ```bash
   cat > .env << EOF
   AIRFLOW_IMAGE=apache/airflow:3.1.5
   AIRFLOW_UID=50000
   AIRFLOW_PROJ_DIR=.
   
   POSTGRES_USER=airflow
   POSTGRES_PASSWORD=airflow
   POSTGRES_DB=airflow
   POSTGRES_HOST=postgres
   
   _AIRFLOW_WWW_USER_USERNAME=airflow
   _AIRFLOW_WWW_USER_PASSWORD=airflow
   EOF
   ```

3. **Build and start services**
   ```bash
   docker-compose up -d
   ```

4. **Access Airflow UI**
   - URL: http://localhost:8080
   - Username: `airflow`
   - Password: `airflow`

### Configuration

#### Set up Airflow Connections

1. **JSearch API Connection** (`jsearch_api`)
   - Conn Type: HTTP
   - Host: `https://jsearch.p.rapidapi.com`
   - Extra (JSON):
     ```json
     {
       "endpoint": "search",
       "key": "YOUR_API_KEY",
       "num_page": "1",
       "country": "fr",
       "posted_at": "today"
     }
     ```

2. **PostgreSQL Connection** (`postgres_job_db`)
   - Conn Type: Postgres
   - Host: `<remote-database-host>`
   - Schema: `<database-name>`
   - Login: `<username>`
   - Password: `<password>`
   - Port: `5432`

## ğŸ“ˆ DAG Workflow

The `job_post_dag` executes daily with the following task sequence:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ is_api_availableâ”‚
â”‚     @task       â”‚
â”‚    .sensor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ API is available
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    extract      â”‚
â”‚     @task       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ extraction complete
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   transform     â”‚
â”‚     @task       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ transformation complete
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      load       â”‚
â”‚     @task       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Task Details

1. **is_api_available**: Sensor that checks API health (60s intervals, 10min timeout)
2. **extract**: Fetches job postings from JSearch API
3. **transform**: Enriches data with skills, benefits, location codes
4. **load**: Inserts data into dimensional data warehouse

### Schedule

- **Frequency**: Daily (`@daily`)
- **Start Date**: January 1, 2026
- **Timezone**: Europe/Paris
- **Catchup**: Disabled
- **Max Consecutive Failures**: 3

## ğŸ“Š Connecting to Superset

Once data is loaded into PostgreSQL, connect Apache Superset (remote) to visualize insights:

1. **Add PostgreSQL Database in Superset**
   - Navigate to Data â†’ Databases â†’ + Database
   - Connection String: `postgresql://<user>:<password>@<host>:<port>/<database>`

2. **Create Datasets**
   - Use `fact_job_post` joined with dimension tables
   - Configure metrics and dimensions

3. **Build Dashboards**
   - Job posting trends over time
   - Top technologies in demand
   - Geographic distribution of opportunities
   - Salary ranges by technology
   - Benefits analysis

## ğŸ” Data Enrichment Details

### Technologies Detected
- Programming languages (Python, Java, SQL, JavaScript, etc.)
- Data tools (Spark, Kafka, Airflow, dbt, etc.)
- Cloud platforms (AWS, Azure, GCP)
- ML/AI frameworks (TensorFlow, PyTorch, scikit-learn)

### Benefits Identified
- Remote work options
- Health insurance (mutuelle)
- Meal vouchers (tickets restaurant)
- RTT (reduced working time)
- Performance bonuses
- 13th-month salary
- CSE (works council benefits)

## ğŸ§ª Testing and Monitoring

### Run Manual DAG Execution
```bash
# Trigger DAG manually
docker-compose exec airflow-scheduler airflow dags trigger job_post_dag
```

### View Logs
```bash
# Scheduler logs
docker-compose logs -f airflow-scheduler

# Worker logs
docker-compose logs -f airflow-worker
```

### Monitor with Flower (Celery UI)
```bash
docker-compose --profile flower up -d
# Access at http://localhost:5555
```

## ğŸ›¡ï¸ Error Handling

- **API Failures**: Sensor retries for 10 minutes before failing
- **Database Errors**: Transactions are rolled back on failure
- **Duplicate Jobs**: Upsert logic prevents duplicates using `job_id`
- **Max Failures**: DAG pauses after 3 consecutive failed runs

## ğŸ“ Development

### Adding New Transformations

Edit [dags/job_post_dag.py](dags/job_post_dag.py) in the `transform` task to add custom logic.

### Extending Data Sources

Add new reference files in the `data/` directory and update transformation logic.

### Custom Airflow Plugins

Place custom operators/sensors in the `plugins/` directory.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

Apache License 2.0

## ğŸ‘¥ Support

For issues, questions, or contributions, please open an issue in the repository.
