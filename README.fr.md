# Plateforme ETL d'Intelligence du MarchÃ© de l'Emploi

[![Version Python](https://img.shields.io/badge/python-3.13%2B-blue.svg)](https://www.python.org/downloads/)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-3.1.5-017CEE?logo=apache-airflow)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-316192?logo=postgresql)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker)](https://www.docker.com/)
[![Licence](https://img.shields.io/badge/Licence-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)

**ğŸŒ Language / Langue:** [English](README.md) | [FranÃ§ais](README.fr.md)

---

Une plateforme complÃ¨te d'ingÃ©nierie des donnÃ©es pour extraire, transformer et charger les donnÃ©es du marchÃ© de l'emploi en utilisant Apache Airflow. Ce projet automatise la collecte d'offres d'emploi depuis des APIs externes, les enrichit avec des compÃ©tences techniques, des avantages et des donnÃ©es de localisation, et les stocke dans un entrepÃ´t de donnÃ©es dimensionnel pour l'intelligence d'affaires et la visualisation.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   JSearch API   â”‚
â”‚  Offres d'emploiâ”‚
â”‚   externes      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Extraction
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache Airflow  â”‚
â”‚  Pipeline ETL   â”‚â—„â”€â”€â”€ Transformation
â”‚  Orchestration  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Chargement
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚
â”‚ EntrepÃ´t de     â”‚
â”‚ donnÃ©es (Ã‰toile)â”‚
â”‚ Base distante   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ RequÃªtes
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Apache Superset  â”‚
â”‚ Visualisation   â”‚
â”‚ Outil BI distantâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants

- **JSearch API** : Source externe de donnÃ©es d'offres d'emploi
- **Apache Airflow** : Moteur d'orchestration pour les workflows ETL
- **PostgreSQL** : EntrepÃ´t de donnÃ©es dimensionnel (schÃ©ma en Ã©toile)
- **Apache Superset** : Plateforme d'intelligence d'affaires et de visualisation de donnÃ©es (distant)

### Infrastructure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Externe    â”‚
â”‚  JSearch API     â”‚
â”‚ (Source donnÃ©es) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ HTTP/REST
         â”‚ Appels API
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          VM 1 (Airflow)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Apache Airflow 3.1.5        â”‚  â”‚
â”‚  â”‚   - Planificateur             â”‚  â”‚
â”‚  â”‚   - Workers (Celery)          â”‚  â”‚
â”‚  â”‚   - Interface Web             â”‚  â”‚
â”‚  â”‚   - Processeur DAG            â”‚  â”‚
â”‚  â”‚   Orchestration ETL           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Connexion
               â”‚ PostgreSQL
               â”‚ (Ã‰criture)
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VM 2 (EntrepÃ´t de donnÃ©es)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   PostgreSQL 16               â”‚  â”‚
â”‚  â”‚   - SchÃ©ma en Ã©toile          â”‚  â”‚
â”‚  â”‚   - Tables de faits           â”‚  â”‚
â”‚  â”‚   - Tables de dimensions      â”‚  â”‚
â”‚  â”‚   Stockage & Gestion          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ RequÃªtes SQL
               â”‚ (Lecture)
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VM 3 (Business Intelligence)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Apache Superset             â”‚  â”‚
â”‚  â”‚   - Tableaux de bord          â”‚  â”‚
â”‚  â”‚   - Graphiques & Visus        â”‚  â”‚
â”‚  â”‚   - Analyses & Rapports       â”‚  â”‚
â”‚  â”‚   Exploration des donnÃ©es     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


## ğŸ“Š ModÃ¨le de DonnÃ©es

La plateforme implÃ©mente un **schÃ©ma en Ã©toile** avec la structure suivante :

```
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚    dim_date      â”‚
                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                           â”‚ date_key (PK)    â”‚
                           â”‚ full_date        â”‚
                           â”‚ year             â”‚
                           â”‚ quarter          â”‚
                           â”‚ month            â”‚
                           â”‚ month_name       â”‚
                           â”‚ day              â”‚
                           â”‚ day_of_week      â”‚
                           â”‚ day_name         â”‚
                           â”‚ week_of_year     â”‚
                           â”‚ is_weekend       â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dim_employer    â”‚                â”‚               â”‚  dim_location    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ employer_key (PK)â”‚                â”‚               â”‚ location_key (PK)â”‚
â”‚ employer_name    â”‚                â”‚               â”‚ job_city         â”‚
â”‚ publisher        â”‚                â”‚               â”‚ job_country      â”‚
â”‚ industry         â”‚                â”‚               â”‚ job_region       â”‚
â”‚ company_size     â”‚                â”‚               â”‚ continent        â”‚
â”‚ founded_year     â”‚                â”‚               â”‚ latitude         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚               â”‚ longitude        â”‚
         â”‚                          â”‚               â”‚ postcode         â”‚
         â”‚                          â”‚               â”‚ isocode3166      â”‚
         â”‚                          â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚                        â”‚
         â”‚                          â–¼                        â”‚
         â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  fact_job_post       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â”‚ job_key (PK)         â”‚
                         â”‚ date_key (FK)        â”‚
                         â”‚ location_key (FK)    â”‚
                         â”‚ employer_key (FK)    â”‚
                         â”‚ job_id               â”‚
                         â”‚ job_title            â”‚
                         â”‚ apply_link           â”‚
                         â”‚ employment_type      â”‚
                         â”‚ posted_timestamp     â”‚
                         â”‚ job_salary           â”‚
                         â”‚ job_min_salary       â”‚
                         â”‚ job_max_salary       â”‚
                         â”‚ technologies_list    â”‚
                         â”‚ tools_list           â”‚
                         â”‚ benefits_list        â”‚
                         â”‚ seniority_levels_listâ”‚
                         â”‚ technology_count     â”‚
                         â”‚ tools_count          â”‚
                         â”‚ benefits_count       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Table de Faits
- `fact_job_post` : Table de faits centrale contenant les mÃ©triques des offres d'emploi et les clÃ©s Ã©trangÃ¨res vers toutes les dimensions

### Tables de Dimensions
- `dim_date` : Dimension temporelle avec hiÃ©rarchies (annÃ©e, trimestre, mois, semaine, jour)
- `dim_location` : Dimension gÃ©ographique (ville, pays, rÃ©gion, code postal, codes ISO)
- `dim_employer` : Dimension employeur/entreprise avec mÃ©tadonnÃ©es

## ğŸš€ FonctionnalitÃ©s

### CapacitÃ©s du Pipeline ETL

1. **Extraction**
   - Capteur de vÃ©rification de santÃ© de l'API pour garantir la disponibilitÃ© de la source de donnÃ©es
   - RÃ©cupÃ©ration automatisÃ©e des offres d'emploi depuis l'API JSearch
   - ParamÃ¨tres de recherche configurables (localisation, plage de dates, nombre de pages)

2. **Transformation**
   - **DÃ©tection des CompÃ©tences Techniques** : Identifie les technologies et outils mentionnÃ©s dans les descriptions d'emploi
     - Paysage Machine Learning & IA (depuis MAD landscape)
     - Langages de programmation et frameworks
     - Outils d'ingÃ©nierie des donnÃ©es
   - **Enrichissement de la Localisation** : 
     - Recherche de code postal depuis les donnÃ©es INSEE
     - GÃ©nÃ©ration du code rÃ©gion ISO 3166-2
   - **Extraction du Niveau de SÃ©nioritÃ©** : DÃ©tecte les exigences d'expÃ©rience
   - **Information Salariale** : Extrait les fourchettes salariales mentionnÃ©es
   - **DÃ©tection des Avantages** : Identifie les avantages comme le tÃ©lÃ©travail, mutuelle, tickets restaurant, etc.

3. **Chargement**
   - ModÃ©lisation dimensionnelle avec clÃ©s de substitution
   - Logique d'upsert (gestion des doublons)
   - Maintien de l'intÃ©gritÃ© rÃ©fÃ©rentielle
   - Gestion des transactions avec rollback en cas d'erreur

## ğŸ› ï¸ Stack Technologique

- **Orchestration** : Apache Airflow 3.1.5
- **Distribution des TÃ¢ches** : Celery avec broker Redis
- **Base de DonnÃ©es** : PostgreSQL 16
- **Visualisation de DonnÃ©es** : Apache Superset (distant)
- **Conteneurisation** : Docker & Docker Compose
- **Langage** : Python 3.13+

## ğŸ“ Structure du Projet

```
job-market-intelligence-etl-platform/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ job_post_dag.py          # DÃ©finition du DAG ETL principal
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mad_landscape.json       # RÃ©fÃ©rence outils ML/IA
â”‚   â”œâ”€â”€ technologies.json        # RÃ©fÃ©rence stack technique
â”‚   â””â”€â”€ post_code_insee.csv      # Codes postaux franÃ§ais
â”œâ”€â”€ config/
â”‚   â””â”€â”€ airflow.cfg              # Configuration Airflow
â”œâ”€â”€ logs/                         # Logs d'exÃ©cution Airflow
â”œâ”€â”€ plugins/                      # Plugins Airflow personnalisÃ©s
â”œâ”€â”€ include/                      # Ressources supplÃ©mentaires
â”œâ”€â”€ docker-compose.yaml          # Orchestration multi-conteneurs
â””â”€â”€ pyproject.toml               # MÃ©tadonnÃ©es du projet Python
```

## ğŸ”§ Installation et Configuration

### PrÃ©requis

- Docker et Docker Compose
- Au moins 4GB de RAM
- Au moins 2 cÅ“urs CPU
- 10GB d'espace disque libre

### Ã‰tapes d'Installation

1. **Cloner le dÃ©pÃ´t**
   ```bash
   git clone <repository-url>
   cd job-market-intelligence-etl-platform
   ```

2. **CrÃ©er le fichier d'environnement**
   ```bash
   cat > .env << EOF
   AIRFLOW_IMAGE=apache/airflow:3.1.5
   AIRFLOW_UID=50000
   AIRFLOW_PROJ_DIR=.
   
   POSTGRES_USER=airflow
   POSTGRES_PASSWORD=airflow
   POSTGRES_DB=airflow
   POSTGRES_HOST=postgres
   
   _AIRFLOW_WWW_USER_USERNAME=airflow
   _AIRFLOW_WWW_USER_PASSWORD=airflow
   EOF
   ```

3. **Construire et dÃ©marrer les services**
   ```bash
   docker-compose up -d
   ```

4. **AccÃ©der Ã  l'interface Airflow**
   - URL : http://localhost:8080
   - Nom d'utilisateur : `airflow`
   - Mot de passe : `airflow`

### Configuration

#### Configurer les Connexions Airflow

1. **Connexion API JSearch** (`jsearch_api`)
   - Type de Conn : HTTP
   - HÃ´te : `https://jsearch.p.rapidapi.com`
   - Extra (JSON) :
     ```json
     {
       "endpoint": "search",
       "key": "VOTRE_CLE_API",
       "num_page": "1",
       "country": "fr",
       "posted_at": "today"
     }
     ```

2. **Connexion PostgreSQL** (`postgres_job_db`)
   - Type de Conn : Postgres
   - HÃ´te : `<hote-base-distante>`
   - SchÃ©ma : `<nom-base>`
   - Login : `<utilisateur>`
   - Mot de passe : `<mot-de-passe>`
   - Port : `5432`

## ğŸ“ˆ Workflow du DAG

Le `job_post_dag` s'exÃ©cute quotidiennement avec la sÃ©quence de tÃ¢ches suivante :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ is_api_availableâ”‚
â”‚     @task       â”‚
â”‚    .sensor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ L'API est disponible
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    extract      â”‚
â”‚     @task       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ extraction terminÃ©e
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   transform     â”‚
â”‚     @task       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ transformation terminÃ©e
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      load       â”‚
â”‚     @task       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©tails des TÃ¢ches

1. **is_api_available** : Capteur qui vÃ©rifie la santÃ© de l'API (intervalles de 60s, timeout de 10min)
2. **extract** : RÃ©cupÃ¨re les offres d'emploi depuis l'API JSearch
3. **transform** : Enrichit les donnÃ©es avec compÃ©tences, avantages, codes de localisation
4. **load** : InsÃ¨re les donnÃ©es dans l'entrepÃ´t de donnÃ©es dimensionnel

### Planification

- **FrÃ©quence** : Quotidienne (`@daily`)
- **Date de DÃ©but** : 1er janvier 2026
- **Fuseau Horaire** : Europe/Paris
- **Catchup** : DÃ©sactivÃ©
- **Ã‰checs ConsÃ©cutifs Max** : 3

## ğŸ“Š Connexion Ã  Superset

Une fois les donnÃ©es chargÃ©es dans PostgreSQL, connectez Apache Superset (distant) pour visualiser les insights :

1. **Ajouter la Base de DonnÃ©es PostgreSQL dans Superset**
   - Naviguer vers Data â†’ Databases â†’ + Database
   - ChaÃ®ne de Connexion : `postgresql://<user>:<password>@<host>:<port>/<database>`

2. **CrÃ©er les Datasets**
   - Utiliser `fact_job_post` jointure avec les tables de dimensions
   - Configurer les mÃ©triques et dimensions

3. **Construire les Tableaux de Bord**
   - Tendances des offres d'emploi dans le temps
   - Technologies les plus demandÃ©es
   - Distribution gÃ©ographique des opportunitÃ©s
   - Fourchettes salariales par technologie
   - Analyse des avantages

## ğŸ” DÃ©tails de l'Enrichissement des DonnÃ©es

### Technologies DÃ©tectÃ©es
- Langages de programmation (Python, Java, SQL, JavaScript, etc.)
- Outils de donnÃ©es (Spark, Kafka, Airflow, dbt, etc.)
- Plateformes Cloud (AWS, Azure, GCP)
- Frameworks ML/IA (TensorFlow, PyTorch, scikit-learn)

### Avantages IdentifiÃ©s
- Options de tÃ©lÃ©travail
- Assurance santÃ© (mutuelle)
- Tickets restaurant
- RTT (rÃ©duction du temps de travail)
- Primes de performance
- 13Ã¨me mois
- Avantages CSE

## ğŸ§ª Tests et Surveillance

### ExÃ©cuter Manuellement le DAG
```bash
# DÃ©clencher le DAG manuellement
docker-compose exec airflow-scheduler airflow dags trigger job_post_dag
```

### Voir les Logs
```bash
# Logs du planificateur
docker-compose logs -f airflow-scheduler

# Logs des workers
docker-compose logs -f airflow-worker
```

### Surveiller avec Flower (Interface Celery)
```bash
docker-compose --profile flower up -d
# AccÃ©der Ã  http://localhost:5555
```

## ğŸ›¡ï¸ Gestion des Erreurs

- **Ã‰checs API** : Le capteur rÃ©essaie pendant 10 minutes avant d'Ã©chouer
- **Erreurs de Base de DonnÃ©es** : Les transactions sont annulÃ©es en cas d'Ã©chec
- **Emplois en Double** : La logique d'upsert empÃªche les doublons en utilisant `job_id`
- **Ã‰checs Max** : Le DAG se met en pause aprÃ¨s 3 exÃ©cutions consÃ©cutives Ã©chouÃ©es

## ğŸ“ DÃ©veloppement

### Ajouter de Nouvelles Transformations

Ã‰ditez [dags/job_post_dag.py](dags/job_post_dag.py) dans la tÃ¢che `transform` pour ajouter une logique personnalisÃ©e.

### Ã‰tendre les Sources de DonnÃ©es

Ajoutez de nouveaux fichiers de rÃ©fÃ©rence dans le rÃ©pertoire `data/` et mettez Ã  jour la logique de transformation.

### Plugins Airflow PersonnalisÃ©s

Placez les opÃ©rateurs/capteurs personnalisÃ©s dans le rÃ©pertoire `plugins/`.

## ğŸ¤ Contribution

1. Forker le dÃ©pÃ´t
2. CrÃ©er une branche de fonctionnalitÃ©
3. Effectuer vos modifications
4. Tester minutieusement
5. Soumettre une pull request

## ğŸ“„ Licence

Apache License 2.0

## ğŸ‘¥ Support

Pour les problÃ¨mes, questions ou contributions, veuillez ouvrir une issue dans le dÃ©pÃ´t.

---
